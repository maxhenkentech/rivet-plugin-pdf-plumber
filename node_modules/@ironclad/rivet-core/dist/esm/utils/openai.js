import { DEFAULT_CHAT_NODE_TIMEOUT } from './defaults.js';
import fetchEventSource from './fetchEventSource.js';
export const openaiModels = {
    'gpt-4': {
        maxTokens: 8192,
        cost: {
            prompt: 0.03,
            completion: 0.06,
        },
        displayName: 'GPT-4',
    },
    'gpt-4-32k': {
        maxTokens: 32768,
        cost: {
            prompt: 0.06,
            completion: 0.12,
        },
        displayName: 'GPT-4 32k',
    },
    'gpt-4-0613': {
        maxTokens: 8192,
        cost: {
            prompt: 0.03,
            completion: 0.06,
        },
        displayName: 'GPT-4 (v0613)',
    },
    'gpt-4-32k-0613': {
        maxTokens: 32768,
        cost: {
            prompt: 0.06,
            completion: 0.12,
        },
        displayName: 'GPT-4 32k (v0613)',
    },
    'gpt-3.5-turbo': {
        maxTokens: 4096,
        cost: {
            prompt: 0.002,
            completion: 0.002,
        },
        displayName: 'GPT-3.5 Turbo',
    },
    'gpt-3.5-turbo-16k': {
        maxTokens: 16384,
        cost: {
            prompt: 0.003,
            completion: 0.004,
        },
        displayName: 'GPT-3.5 16k',
    },
    'gpt-3.5-turbo-0613': {
        maxTokens: 16384,
        cost: {
            prompt: 0.002,
            completion: 0.002,
        },
        displayName: 'GPT-3.5 (v0613)',
    },
    'gpt-3.5-turbo-16k-0613': {
        maxTokens: 16384,
        cost: {
            prompt: 0.003,
            completion: 0.004,
        },
        displayName: 'GPT-3.5 16k (v0613)',
    },
    'gpt-3.5-turbo-0301': {
        maxTokens: 16384,
        cost: {
            prompt: 0.002,
            completion: 0.002,
        },
        displayName: 'GPT-3.5 (v0301)',
    },
    'gpt-4-0314': {
        maxTokens: 8192,
        cost: {
            prompt: 0.03,
            completion: 0.06,
        },
        displayName: 'GPT-4 (v0314)',
    },
    'gpt-4-32k-0314': {
        maxTokens: 32768,
        cost: {
            prompt: 0.06,
            completion: 0.12,
        },
        displayName: 'GPT-4 32k (v0314)',
    },
    'local-model': {
        maxTokens: Number.MAX_SAFE_INTEGER,
        cost: {
            prompt: 0,
            completion: 0,
        },
        displayName: 'Local Model',
    },
};
export const openAiModelOptions = Object.entries(openaiModels).map(([id, { displayName }]) => ({
    value: id,
    label: displayName,
}));
export class OpenAIError extends Error {
    status;
    responseJson;
    constructor(status, responseJson) {
        super(`OpenAIError: ${status} ${JSON.stringify(responseJson)}`);
        this.status = status;
        this.responseJson = responseJson;
        this.name = 'OpenAIError';
    }
}
export async function* streamChatCompletions({ endpoint, auth, signal, headers, timeout, ...rest }) {
    const abortSignal = signal ?? new AbortController().signal;
    const response = await fetchEventSource(endpoint, {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            Authorization: `Bearer ${auth.apiKey}`,
            ...(auth.organization ? { 'OpenAI-Organization': auth.organization } : {}),
            ...headers,
        },
        body: JSON.stringify({
            ...rest,
            stream: true,
        }),
        signal: abortSignal,
    }, timeout ?? DEFAULT_CHAT_NODE_TIMEOUT);
    let hadChunks = false;
    for await (const chunk of response.events()) {
        hadChunks = true;
        if (chunk === '[DONE]' || abortSignal?.aborted) {
            return;
        }
        let data;
        try {
            data = JSON.parse(chunk);
        }
        catch (err) {
            console.error('JSON parse failed on chunk: ', chunk);
            throw err;
        }
        yield data;
    }
    if (!hadChunks) {
        const responseJson = await response.json();
        throw new OpenAIError(response.status, responseJson);
    }
}
