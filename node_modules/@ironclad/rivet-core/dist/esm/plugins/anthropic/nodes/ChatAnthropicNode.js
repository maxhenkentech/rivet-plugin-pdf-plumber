import { anthropicModelOptions, anthropicModels, streamChatCompletions, } from '../anthropic.js';
import { nanoid } from 'nanoid/non-secure';
import { dedent } from 'ts-dedent';
import retry from 'p-retry';
import { match } from 'ts-pattern';
import { coerceType, coerceTypeOptional } from '../../../utils/coerceType.js';
import { expectTypeOptional } from '../../../utils/expectType.js';
import { addWarning } from '../../../utils/outputs.js';
import { getError } from '../../../utils/errors.js';
import { pluginNodeDefinition } from '../../../model/NodeDefinition.js';
import { getScalarTypeOf, isArrayDataValue } from '../../../model/DataValue.js';
// Temporary
const cache = new Map();
export const ChatAnthropicNodeImpl = {
    create() {
        const chartNode = {
            type: 'chatAnthropic',
            title: 'Chat (Anthropic)',
            id: nanoid(),
            visualData: {
                x: 0,
                y: 0,
                width: 275,
            },
            data: {
                model: 'claude-2',
                useModelInput: false,
                temperature: 0.5,
                useTemperatureInput: false,
                top_p: 1,
                useTopPInput: false,
                top_k: undefined,
                useTopKInput: false,
                useTopP: false,
                useUseTopPInput: false,
                maxTokens: 1024,
                useMaxTokensInput: false,
                useStop: false,
                stop: '',
                useStopInput: false,
                cache: false,
                useAsGraphPartialOutput: true,
            },
        };
        return chartNode;
    },
    getInputDefinitions(data) {
        const inputs = [];
        if (data.useModelInput) {
            inputs.push({
                id: 'model',
                title: 'Model',
                dataType: 'string',
                required: false,
            });
        }
        if (data.useTemperatureInput) {
            inputs.push({
                dataType: 'number',
                id: 'temperature',
                title: 'Temperature',
            });
        }
        if (data.useTopPInput) {
            inputs.push({
                dataType: 'number',
                id: 'top_p',
                title: 'Top P',
            });
        }
        if (data.useUseTopPInput) {
            inputs.push({
                dataType: 'boolean',
                id: 'useTopP',
                title: 'Use Top P',
            });
        }
        if (data.useMaxTokensInput) {
            inputs.push({
                dataType: 'number',
                id: 'maxTokens',
                title: 'Max Tokens',
            });
        }
        if (data.useStopInput) {
            inputs.push({
                dataType: 'string',
                id: 'stop',
                title: 'Stop',
            });
        }
        inputs.push({
            dataType: ['chat-message', 'chat-message[]'],
            id: 'prompt',
            title: 'Prompt',
        });
        return inputs;
    },
    getOutputDefinitions(_data) {
        const outputs = [];
        outputs.push({
            dataType: 'string',
            id: 'response',
            title: 'Response',
        });
        return outputs;
    },
    getBody(data) {
        return dedent `
      ${data.model === 'claude-2' ? 'Claude' : 'Claude Instant'}
      ${data.useTopP
            ? `Top P: ${data.useTopPInput ? '(Using Input)' : data.top_p}`
            : `Temperature: ${data.useTemperatureInput ? '(Using Input)' : data.temperature}`}
      Max Tokens: ${data.maxTokens}
      ${data.useStop ? `Stop: ${data.useStopInput ? '(Using Input)' : data.stop}` : ''}
    `;
    },
    getEditors() {
        return [
            {
                type: 'dropdown',
                label: 'Model',
                dataKey: 'model',
                useInputToggleDataKey: 'useModelInput',
                options: anthropicModelOptions,
            },
            {
                type: 'number',
                label: 'Temperature',
                dataKey: 'temperature',
                useInputToggleDataKey: 'useTemperatureInput',
                min: 0,
                max: 2,
                step: 0.1,
            },
            {
                type: 'number',
                label: 'Top P',
                dataKey: 'top_p',
                useInputToggleDataKey: 'useTopPInput',
                min: 0,
                max: 1,
                step: 0.1,
            },
            {
                type: 'toggle',
                label: 'Use Top P',
                dataKey: 'useTopP',
                useInputToggleDataKey: 'useUseTopPInput',
            },
            {
                type: 'number',
                label: 'Max Tokens',
                dataKey: 'maxTokens',
                useInputToggleDataKey: 'useMaxTokensInput',
                min: 0,
                max: Number.MAX_SAFE_INTEGER,
                step: 1,
            },
            {
                type: 'string',
                label: 'Stop',
                dataKey: 'stop',
                useInputToggleDataKey: 'useStopInput',
            },
            {
                type: 'toggle',
                label: 'Cache (same inputs, same outputs)',
                dataKey: 'cache',
            },
            {
                type: 'toggle',
                label: 'Use for subgraph partial output',
                dataKey: 'useAsGraphPartialOutput',
            },
        ];
    },
    getUIData() {
        return {
            infoBoxBody: dedent `
        Makes a call to an Anthropic chat model. The settings contains many options for tweaking the model's behavior.
      `,
            infoBoxTitle: 'Chat (Anthropic) Node',
            contextMenuTitle: 'Chat (Anthropic)',
            group: ['AI'],
        };
    },
    async process(data, inputs, context) {
        const output = {};
        const rawModel = data.useModelInput
            ? coerceTypeOptional(inputs['model'], 'string') ?? data.model
            : data.model;
        const model = rawModel;
        const temperature = data.useTemperatureInput
            ? coerceTypeOptional(inputs['temperature'], 'number') ?? data.temperature
            : data.temperature;
        const topP = data.useTopPInput ? coerceTypeOptional(inputs['top_p'], 'number') ?? data.top_p : data.top_p;
        const useTopP = data.useUseTopPInput
            ? coerceTypeOptional(inputs['useTopP'], 'boolean') ?? data.useTopP
            : data.useTopP;
        const stop = data.useStopInput
            ? data.useStop
                ? coerceTypeOptional(inputs['stop'], 'string') ?? data.stop
                : undefined
            : data.stop;
        const { messages } = getChatAnthropicNodeMessages(inputs);
        let prompt = messages.reduce((acc, message) => {
            if (message.type === 'user') {
                return `${acc}\n\nHuman: ${message.message}`;
            }
            else if (message.type === 'assistant') {
                return `${acc}\n\nAssistant: ${message.message}`;
            }
            return acc;
        }, '');
        prompt += '\n\nAssistant:';
        let { maxTokens } = data;
        const tokenizerInfo = {
            node: context.node,
            model,
            endpoint: undefined,
        };
        const tokenCount = context.tokenizer.getTokenCountForString(prompt, tokenizerInfo);
        if (tokenCount >= anthropicModels[model].maxTokens) {
            throw new Error(`The model ${model} can only handle ${anthropicModels[model].maxTokens} tokens, but ${tokenCount} were provided in the prompts alone.`);
        }
        if (tokenCount + maxTokens > anthropicModels[model].maxTokens) {
            const message = `The model can only handle a maximum of ${anthropicModels[model].maxTokens} tokens, but the prompts and max tokens together exceed this limit. The max tokens has been reduced to ${anthropicModels[model].maxTokens - tokenCount}.`;
            addWarning(output, message);
            maxTokens = Math.floor((anthropicModels[model].maxTokens - tokenCount) * 0.95); // reduce max tokens by 5% to be safe, calculation is a little wrong.
        }
        try {
            return await retry(async () => {
                const options = {
                    prompt,
                    model,
                    temperature: useTopP ? undefined : temperature,
                    top_p: useTopP ? topP : undefined,
                    max_tokens_to_sample: maxTokens,
                    stop_sequences: stop ? [stop] : undefined,
                };
                const cacheKey = JSON.stringify(options);
                if (data.cache) {
                    const cached = cache.get(cacheKey);
                    if (cached) {
                        return cached;
                    }
                }
                const startTime = Date.now();
                const apiKey = context.getPluginConfig('anthropicApiKey');
                const chunks = streamChatCompletions({
                    apiKey: apiKey ?? '',
                    signal: context.signal,
                    ...options,
                });
                const responseParts = [];
                for await (const chunk of chunks) {
                    if (!chunk.completion) {
                        // Could be error for some reason ü§∑‚Äç‚ôÇÔ∏è but ignoring has worked for me so far.
                        continue;
                    }
                    responseParts.push(chunk.completion);
                    output['response'] = {
                        type: 'string',
                        value: responseParts.join('').trim(),
                    };
                    context.onPartialOutputs?.(output);
                }
                const endTime = Date.now();
                if (responseParts.length === 0) {
                    throw new Error('No response from Anthropic');
                }
                output['requestTokens'] = { type: 'number', value: tokenCount };
                const responseTokenCount = context.tokenizer.getTokenCountForString(responseParts.join(''), tokenizerInfo);
                output['responseTokens'] = { type: 'number', value: responseTokenCount };
                // TODO
                // const cost =
                //   getCostForPrompt(completionMessages, model) + getCostForTokens(responseTokenCount, 'completion', model);
                // output['cost' as PortId] = { type: 'number', value: cost };
                const duration = endTime - startTime;
                output['duration'] = { type: 'number', value: duration };
                Object.freeze(output);
                cache.set(cacheKey, output);
                return output;
            }, {
                forever: true,
                retries: 10000,
                maxRetryTime: 1000 * 60 * 5,
                factor: 2.5,
                minTimeout: 500,
                maxTimeout: 5000,
                randomize: true,
                signal: context.signal,
                onFailedAttempt(err) {
                    context.trace(`ChatAnthropicNode failed, retrying: ${err.toString()}`);
                    if (context.signal.aborted) {
                        throw new Error('Aborted');
                    }
                    const { retriesLeft } = err;
                    // TODO
                    // if (!(err instanceof OpenAIError)) {
                    //   return; // Just retry?
                    // }
                    // if (err.status === 429) {
                    //   if (retriesLeft) {
                    //     context.onPartialOutputs?.({
                    //       ['response' as PortId]: {
                    //         type: 'string',
                    //         value: 'OpenAI API rate limit exceeded, retrying...',
                    //       },
                    //     });
                    //     return;
                    //   }
                    // }
                    // // We did something wrong (besides rate limit)
                    // if (err.status >= 400 && err.status < 500) {
                    //   throw new Error(err.message);
                    // }
                },
            });
        }
        catch (error) {
            context.trace(getError(error).stack ?? 'Missing stack');
            throw new Error(`Error processing ChatAnthropicNode: ${error.message}`);
        }
    },
};
export const chatAnthropicNode = pluginNodeDefinition(ChatAnthropicNodeImpl, 'Chat');
export function getChatAnthropicNodeMessages(inputs) {
    const prompt = inputs['prompt'];
    if (!prompt) {
        throw new Error('Prompt is required');
    }
    const messages = match(prompt)
        .with({ type: 'chat-message' }, (p) => [p.value])
        .with({ type: 'chat-message[]' }, (p) => p.value)
        .with({ type: 'string' }, (p) => [
        { type: 'user', message: p.value, function_call: undefined, name: undefined },
    ])
        .with({ type: 'string[]' }, (p) => p.value.map((v) => ({ type: 'user', message: v, function_call: undefined, name: undefined })))
        .otherwise((p) => {
        if (isArrayDataValue(p)) {
            const stringValues = p.value.map((v) => coerceType({
                type: getScalarTypeOf(p.type),
                value: v,
            }, 'string'));
            return stringValues
                .filter((v) => v != null)
                .map((v) => ({ type: 'user', message: v, function_call: undefined, name: undefined }));
        }
        const coercedMessage = coerceType(p, 'chat-message');
        if (coercedMessage != null) {
            return [coercedMessage];
        }
        const coercedString = coerceType(p, 'string');
        return coercedString != null
            ? [{ type: 'user', message: coerceType(p, 'string'), function_call: undefined, name: undefined }]
            : [];
    });
    return { messages };
}
