import { ChatCompletion, ChatCompletionCreateParams, ChatCompletionMessage } from "openai/resources/index.mjs";
export interface CachedLLMParams {
    model: string;
    messages: ChatCompletionMessage[];
    functions?: ChatCompletionCreateParams.Function[];
    function_call?: ChatCompletionCreateParams.FunctionCallOption;
    temperature?: number;
    max_tokens?: number;
}
export interface ChatCache {
    get(params: CachedLLMParams): Promise<ChatCompletion | null>;
    set(params: CachedLLMParams, response: ChatCompletion): Promise<void>;
}
export interface OpenAIAuth {
    openAiApiKey?: string;
    openAiOrganizationId?: string;
}
export declare function cachedChatCompletion(params: CachedLLMParams, options: {
    cache?: ChatCache;
} & OpenAIAuth): Promise<ChatCompletion>;
