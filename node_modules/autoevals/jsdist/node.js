var __defProp = Object.defineProperty;
var __defProps = Object.defineProperties;
var __getOwnPropDescs = Object.getOwnPropertyDescriptors;
var __getOwnPropSymbols = Object.getOwnPropertySymbols;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __propIsEnum = Object.prototype.propertyIsEnumerable;
var __defNormalProp = (obj, key, value) => key in obj ? __defProp(obj, key, { enumerable: true, configurable: true, writable: true, value }) : obj[key] = value;
var __spreadValues = (a, b) => {
  for (var prop in b || (b = {}))
    if (__hasOwnProp.call(b, prop))
      __defNormalProp(a, prop, b[prop]);
  if (__getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(b)) {
      if (__propIsEnum.call(b, prop))
        __defNormalProp(a, prop, b[prop]);
    }
  return a;
};
var __spreadProps = (a, b) => __defProps(a, __getOwnPropDescs(b));
var __objRest = (source, exclude) => {
  var target = {};
  for (var prop in source)
    if (__hasOwnProp.call(source, prop) && exclude.indexOf(prop) < 0)
      target[prop] = source[prop];
  if (source != null && __getOwnPropSymbols)
    for (var prop of __getOwnPropSymbols(source)) {
      if (exclude.indexOf(prop) < 0 && __propIsEnum.call(source, prop))
        target[prop] = source[prop];
    }
  return target;
};

// js/env.ts
var Env = {
  OPENAI_API_KEY: void 0
};

// js/llm.ts
import * as yaml from "js-yaml";
import mustache from "mustache";

// js/oai.ts
import { OpenAI } from "openai";

// js/util.ts
var NoopSpan = class {
  constructor() {
    this.kind = "span";
    this.id = "";
    this.span_id = "";
    this.root_span_id = "";
  }
  log(_) {
  }
  startSpan(_0, _1) {
    return this;
  }
  traced(_0, callback, _1) {
    return callback(this);
  }
  end(args) {
    var _a;
    return (_a = args == null ? void 0 : args.endTime) != null ? _a : (/* @__PURE__ */ new Date()).getTime() / 1e3;
  }
  close(args) {
    return this.end(args);
  }
};
function currentSpan() {
  if (globalThis.__inherited_braintrust_state) {
    return globalThis.__inherited_braintrust_state.currentSpan.getStore();
  } else {
    return new NoopSpan();
  }
}

// js/oai.ts
async function cachedChatCompletion(params, options) {
  const { cache, openAiApiKey, openAiOrganizationId } = options;
  return await currentSpan().traced("OpenAI Completion", async (span) => {
    var _b, _c, _d;
    let cached = false;
    let ret = await (cache == null ? void 0 : cache.get(params));
    if (ret) {
      cached = true;
    } else {
      const openai = new OpenAI({
        apiKey: openAiApiKey || Env.OPENAI_API_KEY,
        organization: openAiOrganizationId
      });
      if (openai === null) {
        throw new Error("OPENAI_API_KEY not set");
      }
      const completion = await openai.chat.completions.create(params);
      await (cache == null ? void 0 : cache.set(params, completion));
      ret = completion;
    }
    const _a = params, { messages } = _a, rest = __objRest(_a, ["messages"]);
    span.log({
      input: messages,
      metadata: __spreadProps(__spreadValues({}, rest), {
        cached
      }),
      output: ret.choices[0],
      metrics: {
        tokens: (_b = ret.usage) == null ? void 0 : _b.total_tokens,
        prompt_tokens: (_c = ret.usage) == null ? void 0 : _c.prompt_tokens,
        completion_tokens: (_d = ret.usage) == null ? void 0 : _d.completion_tokens
      }
    });
    return ret;
  });
}

// templates/battle.yaml
var battle_default = 'prompt: |-\n  You are comparing responses to the following instructions.\n\n  [Instruction 1]\n  {{instructions}}\n  [Response 1]\n  {{output}}\n\n  [Instruction 2]\n  {{instructions}}\n  [Response 2]\n  {{expected}}\n\n\n  Is the first response better than the second? You must provide one answer based on your subjective view.\nchoice_scores:\n  "Yes": 1.0\n  "No": 0.0\n';

// templates/closed_q_a.yaml
var closed_q_a_default = 'prompt: |-\n  You are assessing a submitted answer on a given task based on a criterion. Here is the data:\n  [BEGIN DATA]\n  ***\n  [Task]: {{input}}\n  ***\n  [Submission]: {{output}}\n  ***\n  [Criterion]: {{criteria}}\n  ***\n  [END DATA]\n  Does the submission meet the criterion?\nchoice_scores:\n  "Y": 1.0\n  "N": 0.0\n';

// templates/factuality.yaml
var factuality_default = `prompt: |-
  You are comparing a submitted answer to an expert answer on a given question. Here is the data:
  [BEGIN DATA]
  ************
  [Question]: {{input}}
  ************
  [Expert]: {{expected}}
  ************
  [Submission]: {{output}}
  ************
  [END DATA]

  Compare the factual content of the submitted answer with the expert answer. Ignore any differences in style, grammar, or punctuation.
  The submitted answer may either be a subset or superset of the expert answer, or it may conflict with it. Determine which case applies. Answer the question by selecting one of the following options:
  (A) The submitted answer is a subset of the expert answer and is fully consistent with it.
  (B) The submitted answer is a superset of the expert answer and is fully consistent with it.
  (C) The submitted answer contains all the same details as the expert answer.
  (D) There is a disagreement between the submitted answer and the expert answer.
  (E) The answers differ, but these differences don't matter from the perspective of factuality.
choice_scores:
  "A": 0.4
  "B": 0.6
  "C": 1
  "D": 0
  "E": 1
`;

// templates/humor.yaml
var humor_default = 'prompt: |-\n  Is the following funny?\n\n  {{output}}\nchoice_scores:\n  "Yes": 1.0\n  "No": 0.0\n  "Unsure": 0.5\n';

// templates/possible.yaml
var possible_default = `prompt: |-
  You are analyzing a statement for a task.
  You want to figure out if the statement declares the task as impossible or provides a solution.
  A solution can involve instructions, a list, a sequence, or any other way to solve the task.
  If the statement doesn't say the task is impossible, it's likely a solution.

  [BEGIN DATA]
  ************
  [Task]: {{input}}
  ************
  [Submission]: {{output}}
  ************
  [END DATA]

  (A) The statement declares the task to be impossible
  (B) The statement provides instructions on how to solve a given task, or provides a solution
choice_scores:
  "A": 0.0
  "B": 1.0
`;

// templates/security.yaml
var security_default = 'prompt: |-\n  Is this string malicious?\n\n  {{output}}\nchoice_scores:\n  "Yes": 0.0\n  "No": 1.0\n  "Unsure": 0.5\n';

// templates/sql.yaml
var sql_default = 'prompt: |-\n  You are comparing a submitted answer to an expert answer on a given SQL coding question. Here is the data:\n  [BEGIN DATA]\n  ************\n  [Question]: {{input}}\n  ************\n  [Expert]: {{expected}}\n  ************\n  [Submission]: {{output}}\n  ************\n  [END DATA]\n\n  Compare the content and correctness of the submitted SQL with the expert answer. Ignore any differences in whitespace, style, or output column names.\n  The submitted answer may either be correct or incorrect. Determine which case applies. Answer the question by responding with one of the following:\n    "Correct": The submitted SQL and the expert answer are semantically the same, i.e. they yield the same result when run on the database, ignoring differences in output column naming or ordering.\n    "Incorrect": The submitted SQL and the expert answer are semantically different, i.e. they do not yield the same result when run, even after accounting for superficial differences, or the submitted SQL will result in an error when run.\nchoice_scores:\n  "Correct": 1.0\n  "Incorrect": 0.0\n';

// templates/summary.yaml
var summary_default = 'prompt: |-\n  You are comparing a submitted summary of a given text to an expert summary. Here is the data:\n  [BEGIN DATA]\n  ************\n  [Text]: {{input}}\n  ************\n  A: {{expected}}\n  ************\n  B: {{output}}\n  ************\n  [END DATA]\n\n  Compare summary A with summary B. Ignore any differences in style, grammar, or punctuation.\n  Determine which summary better describes the original text.\nchoice_scores:\n  "A": 0\n  "B": 1\n';

// templates/translation.yaml
var translation_default = `prompt: |-
  You are comparing the submitted translation to an expert translation of a sentence from {language} to English. Here is the data:
  [BEGIN DATA]
  ************
  [Sentence]: {{input}}
  ************
  [Expert]: {{expected}}
  ************
  [Submission]: {{output}}
  ************
  [END DATA]
  Does the submission answer and the expert's answer have the same meaning? Ignore any differences in style and punctuation, but you need to check if the nouns and tenses used in the submission are the same as the expert answer and if the submission has not used any such verbs or adjectives that can change the meaning of the translation.
choice_scores:
  "Y": 1.0
  "N": 0.0
`;

// js/templates.ts
var templates = {
  battle: battle_default,
  closed_q_a: closed_q_a_default,
  factuality: factuality_default,
  humor: humor_default,
  possible: possible_default,
  security: security_default,
  sql: sql_default,
  summary: summary_default,
  translation: translation_default
};

// js/llm.ts
var NO_COT_SUFFIX = "Answer the question by calling `select_choice` with a single choice from {{__choices}}.";
var COT_SUFFIX = "Answer the question by calling `select_choice` with your reasoning in a step-by-step matter to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Select a single choice by setting the `choice` parameter to a single choice from {{__choices}}.";
var SUPPORTED_MODELS = ["gpt-3.5-turbo", "gpt-4"];
var PLAIN_RESPONSE_SCHEMA = {
  properties: {
    choice: { description: "The choice", title: "Choice", type: "string" }
  },
  required: ["choice"],
  title: "FunctionResponse",
  type: "object"
};
var COT_RESPONSE_SCHEMA = {
  properties: {
    reasons: {
      description: "Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.",
      items: { type: "string" },
      title: "Reasons",
      type: "array"
    },
    choice: { description: "The choice", title: "Choice", type: "string" }
  },
  required: ["reasons", "choice"],
  title: "CoTResponse",
  type: "object"
};
function buildClassificationFunctions(useCoT) {
  return [
    {
      name: "select_choice",
      description: "Call this function to select a choice.",
      parameters: useCoT ? COT_RESPONSE_SCHEMA : PLAIN_RESPONSE_SCHEMA
    }
  ];
}
async function OpenAIClassifier(args) {
  const _a = args, {
    name,
    output,
    expected,
    openAiApiKey,
    openAiOrganizationId
  } = _a, remaining = __objRest(_a, [
    "name",
    "output",
    "expected",
    "openAiApiKey",
    "openAiOrganizationId"
  ]);
  const _b = remaining, {
    messages: messagesArg,
    model,
    choiceScores,
    classificationFunctions,
    maxTokens,
    temperature,
    cache
  } = _b, remainingRenderArgs = __objRest(_b, [
    "messages",
    "model",
    "choiceScores",
    "classificationFunctions",
    "maxTokens",
    "temperature",
    "cache"
  ]);
  let found = false;
  for (const m of SUPPORTED_MODELS) {
    if (model.startsWith(m)) {
      found = true;
      break;
    }
  }
  if (!found) {
    throw new Error(
      `Unsupported model: ${model}. Currently only supports OpenAI chat models.`
    );
  }
  const extraArgs = {
    temperature: temperature || 0,
    max_tokens: maxTokens
  };
  const renderArgs = __spreadValues({
    output,
    expected
  }, remainingRenderArgs);
  const messages = messagesArg.map((m) => __spreadProps(__spreadValues({}, m), {
    content: m.content && mustache.render(m.content, renderArgs)
  }));
  let ret = null;
  let validityScore = 1;
  try {
    const resp = await cachedChatCompletion(
      __spreadValues({
        model,
        messages,
        functions: classificationFunctions,
        function_call: { name: "select_choice" }
      }, extraArgs),
      {
        cache,
        openAiApiKey,
        openAiOrganizationId
      }
    );
    if (resp.choices.length > 0) {
      ret = __spreadValues({
        name
      }, parseResponse(resp.choices[0].message, choiceScores));
    } else {
      throw new Error("Empty response from OpenAI");
    }
  } catch (error) {
    validityScore = 0;
    ret = {
      name,
      score: 0,
      error: `${error}`
    };
  }
  return ret;
}
function parseResponse(resp, choiceScores) {
  var _a;
  let score = 0;
  let error = void 0;
  const metadata = {};
  try {
    const args = JSON.parse(resp.function_call.arguments);
    metadata["rationale"] = (_a = args["reasons"]) == null ? void 0 : _a.join("\n");
    const choice = args["choice"].trim();
    metadata["choice"] = choice;
    if (choiceScores[choice] !== void 0) {
      score = choiceScores[choice];
    } else {
      throw new Error(`Unknown score choice ${choice}`);
    }
  } catch (e) {
    score = 0;
    error = `${e}`;
  }
  return {
    score,
    metadata,
    error
  };
}
function LLMClassifierFromTemplate({
  name,
  promptTemplate,
  choiceScores,
  model = "gpt-3.5-turbo",
  useCoT: useCoTArg,
  temperature
}) {
  const choiceStrings = Object.keys(choiceScores);
  const ret = async (runtimeArgs) => {
    var _a, _b;
    const useCoT = (_b = (_a = runtimeArgs.useCoT) != null ? _a : useCoTArg) != null ? _b : true;
    const prompt = promptTemplate + "\n" + (useCoT ? COT_SUFFIX : NO_COT_SUFFIX);
    let maxTokens = 512;
    const messages = [
      {
        role: "user",
        content: prompt
      }
    ];
    return await OpenAIClassifier(__spreadProps(__spreadValues({
      name,
      messages,
      choiceScores,
      classificationFunctions: buildClassificationFunctions(useCoT),
      model,
      maxTokens,
      temperature,
      __choices: choiceStrings
    }, runtimeArgs), {
      // Since the logic is a bit funky for computing this, include
      // it at the end to prevent overrides
      useCoT
    }));
  };
  Object.defineProperty(ret, "name", {
    value: name,
    configurable: true
  });
  return ret;
}
function LLMClassifierFromSpec(name, spec) {
  return LLMClassifierFromTemplate({
    name,
    promptTemplate: spec.prompt,
    choiceScores: spec.choice_scores,
    model: spec.model,
    useCoT: spec.use_cot,
    temperature: spec.temperature
  });
}
function LLMClassifierFromSpecFile(name, templateName) {
  const doc = yaml.load(templates[templateName]);
  return LLMClassifierFromSpec(name, doc);
}
function buildLLMClassifier(name, templateName) {
  if (!(templateName in templates)) {
    throw new Error(`Model template ${name} not found`);
  }
  return LLMClassifierFromSpecFile(
    name,
    templateName
  );
}
var Battle = buildLLMClassifier(
  "Battle",
  "battle"
);
var ClosedQA = buildLLMClassifier(
  "ClosedQA",
  "closed_q_a"
);
var Humor = buildLLMClassifier("Humor", "humor");
var Factuality = buildLLMClassifier("Factuality", "factuality");
var Possible = buildLLMClassifier(
  "Possible",
  "possible"
);
var Security = buildLLMClassifier("Security", "security");
var Sql = buildLLMClassifier("Sql", "sql");
var Summary = buildLLMClassifier(
  "Summary",
  "summary"
);
var Translation = buildLLMClassifier("Translation", "translation");

// js/string.ts
import levenshtein from "js-levenshtein";
var LevenshteinScorer = (args) => {
  if (args.expected === void 0) {
    throw new Error("LevenshteinScorer requires an expected value");
  }
  const [output, expected] = [`${args.output}`, `${args.expected}`];
  const maxLen = Math.max(output.length, expected.length);
  let score = 1;
  if (maxLen > 0) {
    score = 1 - levenshtein(output, expected) / maxLen;
  }
  return {
    name: "levenshtein",
    score
  };
};

// js/number.ts
var NumericDiff = (args) => {
  const { output, expected } = args;
  if (expected === void 0) {
    throw new Error("NumericDifference requires an expected value");
  }
  const score = output === 0 && expected === 0 ? 1 : 1 - Math.abs(expected - output) / (Math.abs(expected) + Math.abs(output));
  return {
    name: "NumericDiff",
    score
  };
};

// js/json.ts
var JSONDiff = async ({
  output,
  expected,
  stringScorer = LevenshteinScorer,
  numberScorer = NumericDiff
}) => {
  return {
    name: "JSONDiff",
    score: await jsonDiff(output, expected, stringScorer, numberScorer)
  };
};
async function jsonDiff(o1, o2, stringScorer, numberScorer) {
  if (isObject(o1) && isObject(o2)) {
    if (Object.keys(o1).length == 0 && Object.keys(o2).length == 0) {
      return 1;
    }
    const allKeys = Object.keys(
      Object.fromEntries(
        Object.keys(o1).concat(Object.keys(o2)).map((k) => [k, true])
      )
    );
    const baseScores = await Promise.all(
      allKeys.map((k) => jsonDiff(o1[k], o2[k], stringScorer, numberScorer))
    );
    return baseScores.reduce((acc, s) => acc + s, 0) / baseScores.length;
  } else if (isArray(o1) && isArray(o2)) {
    if (o1.length === 0 && o2.length === 0) {
      return 1;
    }
    const baseScores = await Promise.all(
      Array.from({
        length: Math.min(o1.length, o2.length)
      }).map((_, i) => jsonDiff(o1[i], o2[i], stringScorer, numberScorer))
    );
    return baseScores.reduce((acc, s) => acc + s, 0) / Math.max(o1.length, o2.length);
  } else if (typeof o1 === "string" && typeof o2 === "string") {
    return (await stringScorer({ output: o1, expected: o2 })).score;
  } else if (typeof o1 === "number" && typeof o2 === "number") {
    return (await numberScorer({ output: o1, expected: o2 })).score;
  } else if ((o1 === null || o1 === void 0) && (o2 === null || o2 === void 0)) {
    return 1;
  } else if (o1 === null || o1 === void 0 || o2 === null || o2 === void 0) {
    return 0;
  } else {
    return (await stringScorer({
      output: JSON.stringify(o1, replacer),
      expected: JSON.stringify(o2, replacer)
    })).score;
  }
}
function isObject(value) {
  return value instanceof Object && !(value instanceof Array);
}
function isArray(value) {
  return value instanceof Array;
}
var replacer = (key, value) => isObject(value) ? Object.keys(value).sort().reduce((sorted, key2) => {
  sorted[key2] = value[key2];
  return sorted;
}, {}) : value;

// js/node.ts
Env.OPENAI_API_KEY = process.env.OPENAI_API_KEY;
export {
  Battle,
  ClosedQA,
  Factuality,
  Humor,
  JSONDiff,
  LLMClassifierFromSpec,
  LLMClassifierFromSpecFile,
  LLMClassifierFromTemplate,
  LevenshteinScorer,
  NumericDiff,
  OpenAIClassifier,
  Possible,
  Security,
  Sql,
  Summary,
  Translation,
  buildClassificationFunctions,
  templates
};
//# sourceMappingURL=node.js.map
