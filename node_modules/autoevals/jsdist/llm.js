"use strict";
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
var __awaiter = (this && this.__awaiter) || function (thisArg, _arguments, P, generator) {
    function adopt(value) { return value instanceof P ? value : new P(function (resolve) { resolve(value); }); }
    return new (P || (P = Promise))(function (resolve, reject) {
        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }
        function rejected(value) { try { step(generator["throw"](value)); } catch (e) { reject(e); } }
        function step(result) { result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected); }
        step((generator = generator.apply(thisArg, _arguments || [])).next());
    });
};
var __rest = (this && this.__rest) || function (s, e) {
    var t = {};
    for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0)
        t[p] = s[p];
    if (s != null && typeof Object.getOwnPropertySymbols === "function")
        for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) {
            if (e.indexOf(p[i]) < 0 && Object.prototype.propertyIsEnumerable.call(s, p[i]))
                t[p[i]] = s[p[i]];
        }
    return t;
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.Translation = exports.Summary = exports.Sql = exports.Security = exports.Possible = exports.Factuality = exports.Humor = exports.ClosedQA = exports.Battle = exports.LLMClassifierFromSpecFile = exports.LLMClassifierFromSpec = exports.LLMClassifierFromTemplate = exports.OpenAIClassifier = exports.buildClassificationFunctions = void 0;
const yaml = __importStar(require("js-yaml"));
const mustache_1 = __importDefault(require("mustache"));
const oai_js_1 = require("./oai.js");
const templates_js_1 = require("./templates.js");
const NO_COT_SUFFIX = "Answer the question by calling `select_choice` with a single choice from {{__choices}}.";
const COT_SUFFIX = "Answer the question by calling `select_choice` with your reasoning in a step-by-step matter to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset. Select a single choice by setting the `choice` parameter to a single choice from {{__choices}}.";
const SUPPORTED_MODELS = ["gpt-3.5-turbo", "gpt-4"];
const PLAIN_RESPONSE_SCHEMA = {
    properties: {
        choice: { description: "The choice", title: "Choice", type: "string" },
    },
    required: ["choice"],
    title: "FunctionResponse",
    type: "object",
};
const COT_RESPONSE_SCHEMA = {
    properties: {
        reasons: {
            description: "Write out in a step by step manner your reasoning to be sure that your conclusion is correct. Avoid simply stating the correct answer at the outset.",
            items: { type: "string" },
            title: "Reasons",
            type: "array",
        },
        choice: { description: "The choice", title: "Choice", type: "string" },
    },
    required: ["reasons", "choice"],
    title: "CoTResponse",
    type: "object",
};
function buildClassificationFunctions(useCoT) {
    return [
        {
            name: "select_choice",
            description: "Call this function to select a choice.",
            parameters: useCoT ? COT_RESPONSE_SCHEMA : PLAIN_RESPONSE_SCHEMA,
        },
    ];
}
exports.buildClassificationFunctions = buildClassificationFunctions;
function OpenAIClassifier(args) {
    return __awaiter(this, void 0, void 0, function* () {
        const { name, output, expected, openAiApiKey, openAiOrganizationId } = args, remaining = __rest(args, ["name", "output", "expected", "openAiApiKey", "openAiOrganizationId"]);
        const { messages: messagesArg, model, choiceScores, classificationFunctions, maxTokens, temperature, cache } = remaining, remainingRenderArgs = __rest(remaining, ["messages", "model", "choiceScores", "classificationFunctions", "maxTokens", "temperature", "cache"]);
        let found = false;
        for (const m of SUPPORTED_MODELS) {
            if (model.startsWith(m)) {
                found = true;
                break;
            }
        }
        if (!found) {
            throw new Error(`Unsupported model: ${model}. Currently only supports OpenAI chat models.`);
        }
        const extraArgs = {
            temperature: temperature || 0,
            max_tokens: maxTokens,
        };
        const renderArgs = Object.assign({ output,
            expected }, remainingRenderArgs);
        const messages = messagesArg.map((m) => (Object.assign(Object.assign({}, m), { content: m.content && mustache_1.default.render(m.content, renderArgs) })));
        let ret = null;
        let validityScore = 1;
        try {
            const resp = yield (0, oai_js_1.cachedChatCompletion)(Object.assign({ model,
                messages, functions: classificationFunctions, function_call: { name: "select_choice" } }, extraArgs), {
                cache,
                openAiApiKey,
                openAiOrganizationId,
            });
            if (resp.choices.length > 0) {
                ret = Object.assign({ name }, parseResponse(resp.choices[0].message, choiceScores));
            }
            else {
                throw new Error("Empty response from OpenAI");
            }
        }
        catch (error) {
            validityScore = 0;
            ret = {
                name,
                score: 0,
                error: `${error}`,
            };
        }
        return ret;
    });
}
exports.OpenAIClassifier = OpenAIClassifier;
function parseResponse(resp, choiceScores) {
    var _a;
    let score = 0;
    let error = undefined;
    const metadata = {};
    try {
        const args = JSON.parse(resp.function_call.arguments);
        metadata["rationale"] = (_a = args["reasons"]) === null || _a === void 0 ? void 0 : _a.join("\n");
        const choice = args["choice"].trim();
        metadata["choice"] = choice;
        if (choiceScores[choice] !== undefined) {
            score = choiceScores[choice];
        }
        else {
            throw new Error(`Unknown score choice ${choice}`);
        }
    }
    catch (e) {
        score = 0;
        error = `${e}`;
    }
    return {
        score,
        metadata,
        error,
    };
}
function LLMClassifierFromTemplate({ name, promptTemplate, choiceScores, model = "gpt-3.5-turbo", useCoT: useCoTArg, temperature, }) {
    const choiceStrings = Object.keys(choiceScores);
    const ret = (runtimeArgs) => __awaiter(this, void 0, void 0, function* () {
        var _a, _b;
        const useCoT = (_b = (_a = runtimeArgs.useCoT) !== null && _a !== void 0 ? _a : useCoTArg) !== null && _b !== void 0 ? _b : true;
        const prompt = promptTemplate + "\n" + (useCoT ? COT_SUFFIX : NO_COT_SUFFIX);
        let maxTokens = 512;
        const messages = [
            {
                role: "user",
                content: prompt,
            },
        ];
        return yield OpenAIClassifier(Object.assign(Object.assign({ name,
            messages,
            choiceScores, classificationFunctions: buildClassificationFunctions(useCoT), model,
            maxTokens,
            temperature, __choices: choiceStrings }, runtimeArgs), { 
            // Since the logic is a bit funky for computing this, include
            // it at the end to prevent overrides
            useCoT }));
    });
    Object.defineProperty(ret, "name", {
        value: name,
        configurable: true,
    });
    return ret;
}
exports.LLMClassifierFromTemplate = LLMClassifierFromTemplate;
function LLMClassifierFromSpec(name, spec) {
    return LLMClassifierFromTemplate({
        name,
        promptTemplate: spec.prompt,
        choiceScores: spec.choice_scores,
        model: spec.model,
        useCoT: spec.use_cot,
        temperature: spec.temperature,
    });
}
exports.LLMClassifierFromSpec = LLMClassifierFromSpec;
function LLMClassifierFromSpecFile(name, templateName) {
    const doc = yaml.load(templates_js_1.templates[templateName]);
    return LLMClassifierFromSpec(name, doc);
}
exports.LLMClassifierFromSpecFile = LLMClassifierFromSpecFile;
function buildLLMClassifier(name, templateName) {
    if (!(templateName in templates_js_1.templates)) {
        throw new Error(`Model template ${name} not found`);
    }
    return LLMClassifierFromSpecFile(name, templateName);
}
/**
 * Test whether an output _better_ performs the `instructions` than the original
 * (expected) value.
 */
exports.Battle = buildLLMClassifier("Battle", "battle");
/**
 * Test whether an output answers the `input` using knowledge built into the model.
 * You can specify `criteria` to further constrain the answer.
 */
exports.ClosedQA = buildLLMClassifier("ClosedQA", "closed_q_a");
/**
 * Test whether an output is funny.
 */
exports.Humor = buildLLMClassifier("Humor", "humor");
/**
 * Test whether an output is factual, compared to an original (`expected`) value.
 */
exports.Factuality = buildLLMClassifier("Factuality", "factuality");
/**
 * Test whether an output is a possible solution to the challenge posed in the input.
 */
exports.Possible = buildLLMClassifier("Possible", "possible");
/**
 * Test whether an output is malicious.
 */
exports.Security = buildLLMClassifier("Security", "security");
/**
 * Test whether a SQL query is semantically the same as a reference (output) query.
 */
exports.Sql = buildLLMClassifier("Sql", "sql");
/**
 * Test whether an output is a better summary of the `input` than the original (`expected`) value.
 */
exports.Summary = buildLLMClassifier("Summary", "summary");
/**
 * Test whether an `output` is as good of a translation of the `input` in the specified `language`
 * as an expert (`expected`) value.
 */
exports.Translation = buildLLMClassifier("Translation", "translation");
